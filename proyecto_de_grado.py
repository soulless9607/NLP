# -*- coding: utf-8 -*-
"""Proyecto de Grado.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16zsu6dFByvPlUwHlIxx127KFJ6zgEo79
"""

#@title <- Importación de librerías necesarias

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
#for text pre-processing
import re, string
import os
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from traitlets.utils import text

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
#for model-building
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix

# bag of words
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

!pip install removeaccents
from removeaccents import removeaccents

import pandas as pd
import io 
from google.colab import files   
  
uploaded = files.upload()

#@title <- DATASET ONTOLOGIA

df_ontologia = pd.read_excel('dataset palabras.xlsx') 
df_ontologia.head()

from matplotlib import pyplot as plt
plt.figure(figsize=(10,5))
df_ontologia['Clasificacion'].value_counts().plot(kind='bar');

def print_plot(index):
    example = df_ontologia[df_ontologia.index == index][['Respuesta ', 'Clasificacion']].values[0]
    if len(example) > 0:
        print('Tag:', example[1])
        print(example[0])
        print(' ')
        
for i  in range(170,210):
  print_plot(i)

#@title <- Limpieza y preprocesamiento de ontología

from nltk.tokenize import RegexpTokenizer

def clean_text(text):
    text=re.sub('<.*?>', ' ', text)  
    text = text.translate(str.maketrans(' ',' ',string.punctuation))  
    text = re.sub("\n"," ",text)
    text = text.lower()
    text=' '.join(text.split())
    removed_accent = removeaccents.remove_accents(text)
    text = removed_accent
    return text


cleaned_text_array = []

for i in df_ontologia['Respuesta ']:
    text = clean_text(i)
    cleaned_text_array.append(text)
    

df_ontologia['Cleaned ontology'] = pd.Series(cleaned_text_array)

df_ontologia['Cleaned ontology'].dropna()
df_ontologia['Cleaned ontology']

tokenizer = RegexpTokenizer(r'\w+')
tokenized_data = df_ontologia['Respuesta '].to_string()
filtered_words_1 = tokenizer.tokenize(tokenized_data)


nltk.download('stopwords')
stop_words=set(stopwords.words("spanish"))

ontology_array = []

def filter_words(filtered_words_n, array):   
  for w in filtered_words_n:
    if w not in stop_words and w.isalpha() and w !='NaN':
      removed_accent = removeaccents.remove_accents(w.lower())
      array.append(removed_accent)
  return array

filter_words(filtered_words_1, ontology_array)

print(ontology_array)

df_ontologia

from wordcloud import WordCloud


words = " ".join(df_ontologia['Cleaned ontology'])
wordcloud = WordCloud(stopwords=stop_words, background_color='black', width=2500, height=2000).generate(words)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- Clasificación Naive Bayes ontología
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

DV = "Clasificacion"
X = df_ontologia.drop([DV], axis = 1) 
Y = df_ontologia[DV]

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.6, random_state=1000)

count_vect = CountVectorizer(max_features = 5000) 

X_train_counts = count_vect.fit_transform(X_train['Cleaned ontology']) 

X_test = count_vect.transform(X_test['Cleaned ontology'])


from sklearn.metrics import accuracy_score

Naive = MultinomialNB() 
Naive.fit(X_train_counts, Y_train)
predictions_NB = Naive.predict(X_test)
print("Accuracy Score NB:", accuracy_score(predictions_NB, Y_test)*100,"%")

from sklearn.metrics import classification_report

Naive = MultinomialNB() 
Naive.fit(X_train_counts, Y_train)
predictions_NB = Naive.predict(X_test)
print("                  -----------Classification Report NB ----------------")
print(classification_report(predictions_NB, Y_test))

from sklearn.metrics import confusion_matrix
print("Matriz de confusión NB Ontología:")
confusion_matrix(predictions_NB, Y_test)

#@title <- Clasificación Regresión Logistica Ontología


vectorizer = CountVectorizer(min_df=0, lowercase=False)
vectorizer.fit(df_ontologia['Cleaned ontology'])
vectorizer.transform(df_ontologia['Clasificacion']).toarray()

sentences = df_ontologia['Cleaned ontology'].values
y = df_ontologia['Clasificacion'].values

sentences_train, sentences_test, y_train, y_test = train_test_split(
   sentences, y, test_size=0.6, random_state=1000)


vectorizer = CountVectorizer()
vectorizer.fit(sentences_train)

X_train = vectorizer.transform(sentences_train)
X_test  = vectorizer.transform(sentences_test)


classifier_logReg = LogisticRegression()
classifier_logReg.fit(X_train, y_train)
predictions_LR = classifier_logReg.predict(X_test)

print("Accuracy Score LR:", accuracy_score(predictions_LR, y_test)*100,"%")

print("                  -----------Classification Report LR ----------------")

print(classification_report(predictions_LR, y_test))

from sklearn.metrics import confusion_matrix
print("Matriz de confusión LR Ontología:")
confusion_matrix(predictions_LR, y_test)

from sklearn.model_selection import KFold  
kf = KFold(n_splits=3)
dataset = df_ontologia
for train_index, test_index in kf.split(dataset):
    print(train_index, test_index)

#@title <- Comparación resultados ontología con predicciones

df_ontologia["Predicciones LR"] = pd.Series(predictions_LR)
df_ontologia["Predicciones NB"] = pd.Series(predictions_NB)


df_ontologia = df_ontologia[['Respuesta ', 'Cleaned ontology', 'Clasificacion', 'Predicciones LR', 'Predicciones NB']]

df_ontologia.head(20)

uploaded = files.upload()

#@title <- DATASET USP (Unidad Servicios Psicológicos)
df_usp = pd.read_excel('dataset usp.xlsx') 
df_usp.head()

from matplotlib import pyplot as plt
plt.figure(figsize=(10,5))
df_usp['Clasificacion USP'].value_counts().plot(kind='bar');

#@title <- BALANCEO DATASET

df_usp.drop(df_usp[df_usp['Clasificacion USP'] == 'Rumiación'].index[50: ], inplace = True)

df = df_usp.dropna()

df_usp = df.reset_index(drop=True)

plt.figure(figsize=(10,5))
df_usp['Clasificacion USP'].value_counts().plot(kind='bar');

def print_plot(index):
    example = df_usp[df_usp.index == index][['Respuesta ', 'Clasificacion USP']].values[0]
    if len(example) > 0:
        print('Tag:', example[1])
        print(example[0])
        print(' ')
        
for i  in range(150,170):
  print_plot(i)



cleaned_usp_array = []

for i in df_usp['Respuesta ']:
    text = clean_text(i)
    cleaned_usp_array.append(text)
    

df_usp['Cleaned text'] = pd.Series(cleaned_usp_array)

df_usp['Cleaned text'].dropna()
df_usp['Cleaned text']

from wordcloud import WordCloud

nltk.download('stopwords')
stop_words=set(stopwords.words("spanish"))

words = " ".join(df_usp['Cleaned text'])
wordcloud = WordCloud(stopwords=stop_words, background_color='black', width=2500, height=2000).generate(words)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- Clasificación Naive Bayes USP

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

DV = "Clasificacion USP"
X = df_usp.drop([DV], axis = 1) 
Y = df_usp[DV]

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.6, random_state=1000)

count_vect = CountVectorizer(max_features = 5000) 

X_train_counts = count_vect.fit_transform(X_train['Cleaned text']) 

X_test = count_vect.transform(X_test['Cleaned text'])


from sklearn.metrics import accuracy_score

Naive = MultinomialNB() 
Naive.fit(X_train_counts, Y_train)
predictions_NB = Naive.predict(X_test)
print("Accuracy Score NB:", accuracy_score(predictions_NB, Y_test)*100,"%")

from sklearn.metrics import classification_report

Naive = MultinomialNB() 
Naive.fit(X_train_counts, Y_train)
predictions_NB_usp = Naive.predict(X_test)
print("                  -----------Classification Report NB ----------------")
print(classification_report(predictions_NB_usp, Y_test))

from sklearn.metrics import confusion_matrix
print("Matriz de confusión NB USP:")
confusion_matrix(predictions_NB, Y_test)

kf = KFold(n_splits=3)
dataset = df_usp
for train_index, test_index in kf.split(dataset):
    print(train_index, test_index)

kf = KFold(n_splits=3)
dataset = df_usp
for train_index, test_index in kf.split(dataset):
    print(train_index, test_index)

vectorizer = CountVectorizer(min_df=0, lowercase=False)
vectorizer.fit(df_usp['Cleaned text'])
vectorizer.transform(df_usp['Clasificacion USP']).toarray()

sentences = df_usp['Cleaned text'].values
y = df_usp['Clasificacion USP'].values

sentences_train, sentences_test, y_train, y_test = train_test_split(
   sentences, y, test_size=0.6, random_state=1000)


vectorizer = CountVectorizer()
vectorizer.fit(sentences_train)

X_train = vectorizer.transform(sentences_train)
X_test  = vectorizer.transform(sentences_test)


classifier_logReg = LogisticRegression()
classifier_logReg.fit(X_train, y_train)
predictions_LR_usp = classifier_logReg.predict(X_test)

print("Accuracy Score LR:", accuracy_score(predictions_LR_usp, y_test)*100,"%")

print("                  -----------Classification Report LR ----------------")

print(classification_report(predictions_LR_usp, y_test))

from sklearn.metrics import confusion_matrix
print("Matriz de confusión LR USP:")
confusion_matrix(predictions_LR_usp, Y_test)

df_usp["Predicciones LR"] = pd.Series(predictions_LR_usp)
df_usp["Predicciones NB"] = pd.Series(predictions_NB_usp)


df_train = df_usp[['Respuesta ', 'Cleaned text', 'Clasificacion USP', 'Predicciones LR', 'Predicciones NB']]

df_train